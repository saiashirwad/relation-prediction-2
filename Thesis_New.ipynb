{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Thesis New",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "153rcshd9spORr8e7LGR30RIj9QO6Q9GR",
      "authorship_tag": "ABX9TyMyV9gpQDxB7X/M8aFpduhb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saiashirwad/relation-prediction-2/blob/master/Thesis_New.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwOt5ZmrBd6H",
        "colab_type": "code",
        "outputId": "ad31e7f6-9efb-43f6-9c6d-9bb6d1918ca5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "cd /content/drive/My Drive/Thesis/code/relation-prediction-2"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Thesis/code/relation-prediction-2\n",
            "/content/drive/My Drive/Thesis/code/relation-prediction-2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6s8Y8rPhgP3J",
        "colab_type": "code",
        "outputId": "b53e19a2-faf6-44d4-a1bf-8d2bfdbf22cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pip install torchkge --quiet"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |███████▊                        | 10kB 36.9MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 20kB 3.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 30kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 40kB 3.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 2.6MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8_iqMZLg50K",
        "colab_type": "code",
        "outputId": "69c21f3f-04b6-4a26-b64c-28f38912d44c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pip install torch-scatter==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.4.0.html --quiet"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 10.6MB 1.3MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmJPSc0UBtfm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e24f562e-c787-4740-ddbd-de36b76f8769"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNKesfEsCzTT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import importlib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZFrEivVKqGe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from layers import SpecialSpmmFinal"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aN9wK09tB5iq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import train \n",
        "importlib.reload(train)\n",
        "from train import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iolNZ4gQByT0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn \n",
        "\n",
        "from functools import reduce\n",
        "from operator import mul\n",
        "\n",
        "from torch_scatter import scatter\n",
        "from embedding import EmbeddingMul2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjheHvi2A_7y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kg_train, kg_test, kg_val = load_fb15k237()\n",
        "args = Args(100, 200, 100, 2, 100, 2000, 0.001, 10, 'cuda', 'sgd')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckBArh1zA_71",
        "colab_type": "code",
        "outputId": "b1479caa-f0fb-4fef-b5e4-961069d2b8ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "n_ent, n_rel = kg_train.n_ent, kg_train.n_rel\n",
        "total_triplets = get_valid_triplets(kg_train, kg_test, kg_val)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique triplets: 620232\n",
            "Number of unique triplets: 620232\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWBaZFunA_79",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataloader = DataLoader(kg_train, batch_size=args.batch_size, shuffle=False, pin_memory=cuda.is_available())\n",
        "ent_embed, rel_embed = get_init_embed()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sawtOs2vKAaz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class KGLayer(nn.Module):\n",
        "    def __init__(self, n_entities, n_relations, ent_embed, rel_embed, in_dim, out_dim, concat=True, device=\"gpu\"):\n",
        "        super(KGLayer, self).__init__()\n",
        "\n",
        "        self.n_entities = n_entities\n",
        "        self.n_relations = n_relations\n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.device = device\n",
        "\n",
        "        self.a = nn.Linear(3 * in_dim, out_dim).to(device)\n",
        "        nn.init.xavier_normal_(self.a.weight.data, gain=1.414)\n",
        "\n",
        "        self.concat = concat\n",
        "\n",
        "        self.a_2 = nn.Linear(out_dim, 1).to(device)\n",
        "        nn.init.xavier_normal_(self.a_2.weight.data, gain=1.414)\n",
        "\n",
        "        self.special_spmm_final = SpecialSpmmFinal()\n",
        "\n",
        "        # self.ent_embed = EmbeddingMul2(n_entities, in_dim, ent_embed, True, torch.device(device))\n",
        "        # self.rel_embed = EmbeddingMul2(n_relations, in_dim, rel_embed, True, torch.device(device))\n",
        "\n",
        "        self.ent_embed = nn.Embedding(n_entities, in_dim).to(device)\n",
        "        self.rel_embed = nn.Embedding(n_relations, in_dim).to(device)\n",
        "    \n",
        "    def forward(self, triplets):\n",
        "        N = self.n_entities\n",
        "\n",
        "        h = torch.cat((\n",
        "            self.ent_embed(triplets[:, 0]),\n",
        "            self.ent_embed(triplets[:, 1]),\n",
        "            self.rel_embed(triplets[:, 2])\n",
        "        ), dim=1)\n",
        "        c = self.a(h)\n",
        "        b = F.leaky_relu(self.a_2(c))\n",
        "        e_b = torch.exp(b)\n",
        "\n",
        "        temp = triplets.t()\n",
        "        edges = torch.stack([temp[0], temp[1]])\n",
        "\n",
        "        e_b_sum = self.special_spmm_final(edges, e_b, N, e_b.shape[0], 1)\n",
        "        temp1 = e_b * c\n",
        "\n",
        "        h_sum = self.special_spmm_final(edges, temp1,  N, e_b.shape[0], self.out_dim)\n",
        "\n",
        "        hs = h_sum\n",
        "        ebs = e_b_sum\n",
        "        ebs[ebs == 0] = 1e-12\n",
        "\n",
        "\n",
        "        h_ent = hs / ebs\n",
        "\n",
        "        index = triplets[:, 2]\n",
        "        h_rel = scatter(temp1, index=index, dim=0, reduce=\"mean\")\n",
        "\n",
        "        # del h \n",
        "        # torch.cuda.empty_cache()\n",
        "\n",
        "        if self.concat:\n",
        "            return F.elu(h_ent), F.elu(h_rel)\n",
        "        else:\n",
        "            return h_ent, h_rel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsiRp-VUVTTP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = KGLayer(n_ent, n_rel, ent_embed, rel_embed, 100, 100, True, \"cuda\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQrOXUj9bwLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = SGD(model.parameters(), lr=args.lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgLg-QNoRQVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batches = [b for b in dataloader]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMxXdo4oWX21",
        "colab_type": "code",
        "outputId": "0f53b4f0-1c9d-4c20-f15e-6ea28405d832",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "for epoch in range(10):\n",
        "    losses = []\n",
        "    for batch in batches:\n",
        "        triplets = torch.stack(batch)\n",
        "        triplets, labels, nodes, edges = negative_sampling(triplets, n_ent, args.negative_rate)\n",
        "        triplets, labels = triplets.to(args.device), labels.to(args.device)\n",
        "    \n",
        "        model.zero_grad()\n",
        "    \n",
        "        # start = time.time()\n",
        "        model.train()\n",
        "        ent_embed_, rel_embed_ = model(triplets)\n",
        "        loss = loss_func2(triplets, args.negative_rate, ent_embed_, rel_embed_, device=\"cuda\")\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "        # del triplets\n",
        "        # del labels\n",
        "        # del ent_embed_\n",
        "        # del rel_embed_\n",
        "        # del nodes \n",
        "        # del edges  \n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "        losses.append(loss.item())\n",
        "        # print(loss.item())\n",
        "        # del loss \n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "    print(sum(losses) / len(losses))"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.098989646800243\n",
            "4.1067340582826715\n",
            "4.08440189291961\n",
            "4.084533172802334\n",
            "4.08315485585345\n",
            "4.073859063378216\n",
            "4.065666085612165\n",
            "4.06729426871251\n",
            "4.067413505846567\n",
            "4.057592270148062\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "cfb591f4-66cf-44ad-c565-e06c7bb55fc8",
        "id": "sGfWNssqkacl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "model.eval()\n",
        "validate(model, kg_val, total_triplets, 100, 'cuda')"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Head Rank Mean : 10792.345 | Hits@10 : 0\n",
            "Tail Rank Mean : 10638.13 | Hits@10 : 0\n",
            "Filtered Head Rank Mean: 10792.345\n",
            "Filtered Tail Rank MEan: 10638.13\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGOagQiCcecW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import Set, Tuple\n",
        "def validate(model: KGLayer, kg: KnowledgeGraph, total_triplets: Set[Tuple], batch_size=1000, device='cuda'):\n",
        "    batch_size = 1\n",
        "\n",
        "\n",
        "    n_ranks = 10\n",
        "    k = kg.n_ent\n",
        "\n",
        "    dataloader = DataLoader(kg, batch_size, shuffle=True)\n",
        "\n",
        "    hits = [[] for _ in range(10)]\n",
        "    ranks = [] \n",
        "\n",
        "    rankHs, rankTs = [], []\n",
        "\n",
        "    head_rank_mean, tail_rank_mean, filtered_head_rank_mean, filtered_tail_rank_mean = [0] * 4\n",
        "\n",
        "    head_hits_10_raw, head_hits_10_filter, tail_hits_10_raw, tail_hits_10_filter = [0] * 4\n",
        "\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        if i > 300:\n",
        "            break\n",
        "        \n",
        "        src, dst, rel = batch\n",
        "\n",
        "        if device == 'cuda':\n",
        "            src = src.to(device)\n",
        "            dst = dst.to(device)\n",
        "            rel = rel.to(device)\n",
        "\n",
        "        src = model.ent_embed(src)\n",
        "        dst = model.ent_embed(dst)\n",
        "        rel = model.ent_embed(rel)\n",
        "\n",
        "        loss = torch.norm(src + rel - dst, 2, 1)\n",
        "        loss = loss.repeat(kg.n_ent)\n",
        "\n",
        "\n",
        "        src_ = src.repeat(kg.n_ent, 1)\n",
        "        dst_ = dst.repeat(kg.n_ent, 1)\n",
        "        rel_ = rel.repeat(kg.n_ent, 1)\n",
        "\n",
        "        dist_head_prediction = model.ent_embed.weight + rel_ - dst_\n",
        "        dist_tail_prediction = src_ + rel_ - model.ent_embed.weight\n",
        "\n",
        "        _, head_prediction = torch.topk(torch.sum(torch.abs(dist_head_prediction), dim=1), k=k)\n",
        "        _, tail_prediction = torch.topk(torch.sum(torch.abs(dist_tail_prediction), dim=1), k=k)\n",
        "\n",
        "        head_prediction = head_prediction.cpu()\n",
        "        tail_prediction = tail_prediction.cpu()\n",
        "\n",
        "        head_rank_raw, tail_rank_raw, head_rank_filter, tail_rank_filter = [0] * 4\n",
        "\n",
        "\n",
        "        s, d, r = [b[0] for b in batch] # Ugh\n",
        "\n",
        "        for candidate in head_prediction:\n",
        "            if candidate == s:\n",
        "                break\n",
        "            else:\n",
        "                head_rank_raw += 1 \n",
        "                if (candidate, d, r) in total_triplets:\n",
        "                    continue\n",
        "                else:\n",
        "                    head_rank_filter += 1\n",
        "        \n",
        "        for candidate in tail_prediction:\n",
        "            if candidate == d:\n",
        "                break \n",
        "            else:\n",
        "                tail_rank_raw += 1\n",
        "                if (s, candidate, r) in total_triplets:\n",
        "                    continue\n",
        "                else:\n",
        "                    tail_rank_filter += 1 \n",
        "\n",
        "        head_rank_mean += head_rank_raw\n",
        "        tail_rank_mean += tail_rank_raw\n",
        "\n",
        "        filtered_head_rank_mean += head_rank_filter\n",
        "        filtered_tail_rank_mean += tail_rank_filter\n",
        "\n",
        "        if head_rank_raw < 10:\n",
        "            head_hits_10_raw += 1\n",
        "        \n",
        "        if tail_rank_raw < 10:\n",
        "            tail_hits_10_raw += 1\n",
        "\n",
        "        if head_rank_filter < 10:\n",
        "            head_hits_10_filter += 1\n",
        "        \n",
        "        if tail_rank_filter < 10:\n",
        "            tail_hits_10_filter += 1\n",
        "        \n",
        "    \n",
        "    head_rank_mean /= 200\n",
        "    tail_rank_mean /= 200\n",
        "\n",
        "    filtered_head_rank_mean /= 200\n",
        "    filtered_tail_rank_mean /= 200\n",
        "\n",
        "    print(f'Head Rank Mean : {head_rank_mean} | Hits@10 : {head_hits_10_raw}')\n",
        "    print(f'Tail Rank Mean : {tail_rank_mean} | Hits@10 : {tail_hits_10_raw}')\n",
        "\n",
        "    print(f'Filtered Head Rank Mean: {filtered_head_rank_mean}')\n",
        "    print(f'Filtered Tail Rank MEan: {filtered_tail_rank_mean}')\n",
        "\n",
        "\n",
        "    print()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXv3kkb1uH2H",
        "colab_type": "code",
        "outputId": "35b2c528-d3b7-44a3-c093-379e67e2020f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "model.eval()\n",
        "validate(model, kg_val, total_triplets, 100, 'cuda')"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Head Rank Mean : 137.995 | Hits@10 : 0\n",
            "Tail Rank Mean : 96.18 | Hits@10 : 0\n",
            "Filtered Head Rank Mean: 137.995\n",
            "Filtered Tail Rank MEan: 96.18\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTzLoV_6u5GT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "aaedeea0-9d32-4da1-998a-554d94491ff9"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Apr  8 15:35:19 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    31W / 250W |    875MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsreyYm_TIGX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "8f02b5e1-1cb5-489e-d3d4-f0a054757952"
      },
      "source": [
        "dataloader = DataLoader(kg_val, 1, shuffle=True)\n",
        "data = [d for d  in dataloader]\n",
        "d = data[0]\n",
        "model.eval()"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KGLayer(\n",
              "  (a): Linear(in_features=300, out_features=100, bias=True)\n",
              "  (a_2): Linear(in_features=100, out_features=1, bias=True)\n",
              "  (special_spmm_final): SpecialSpmmFinal()\n",
              "  (ent_embed): Embedding(14541, 100)\n",
              "  (rel_embed): Embedding(237, 100)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-fdvyGQYjbE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "head = model.ent_embed(d[0].to(\"cuda\"))\n",
        "tail = model.ent_embed(d[1].to(\"cuda\"))\n",
        "rel  = model.rel_embed(d[2].to(\"cuda\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1DHeb7QZWWb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_loss = torch.norm(head + rel - tail, 2).repeat(n_ent, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NRySo_JaL9-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tmphead = head.repeat(n_ent, 1)\n",
        "tmptail = tail.repeat(n_ent, 1)\n",
        "tmprel = rel.repeat(n_ent, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIfid7-NaP27",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tmpHloss = torch.norm(model.ent_embed.weight + tmprel - tmptail, 2, 1).view(-1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjkq6yfSasac",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "fc4bfc00-c2e4-4462-c688-e94a840656eb"
      },
      "source": [
        "tmpHloss"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[15.2286],\n",
              "        [13.6497],\n",
              "        [17.3007],\n",
              "        ...,\n",
              "        [15.7120],\n",
              "        [15.6428],\n",
              "        [14.6578]], device='cuda:0', grad_fn=<ViewBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYPhn8wRa0sb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tmpTloss = torch.norm(tmphead + rel - model.ent_embed.weight, 2, 1).view(-1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6BXnPQObCTM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "ff8d6085-4107-4b3c-f2a2-9b5afa6bea1b"
      },
      "source": [
        "tmpTloss"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[16.9137],\n",
              "        [16.8444],\n",
              "        [17.3527],\n",
              "        ...,\n",
              "        [16.3008],\n",
              "        [16.8269],\n",
              "        [17.7307]], device='cuda:0', grad_fn=<ViewBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6meIkLybDuU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rankH = torch.nonzero(F.relu(target_loss - tmpHloss)).size()[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67-ih0XxbWZz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5aab253e-dcec-4707-adac-f75fa042e91e"
      },
      "source": [
        "rankH"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11215"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7jonoNgb_Uj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rankT = torch.nonzero(F.relu(target_loss - tmpTloss)).size()[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8S7xQu_cF_D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e3210cc2-9934-4355-f444-ae7e551342c9"
      },
      "source": [
        "rankT"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2449"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAwcG368cHP7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fast_validate(model, kg, total_triplets, device):\n",
        "    dataloader = DataLoader(kg, 1, shuffle=True)\n",
        "\n",
        "    ranks = []\n",
        "\n",
        "    for i, triplet in enumerate(dataloader):\n",
        "        if i < 1000:\n",
        "            head = model.ent_embed(triplet[0].to(\"cuda\"))\n",
        "            tail = model.ent_embed(triplet[1].to(\"cuda\"))\n",
        "            rel  = model.rel_embed(triplet[2].to(\"cuda\"))\n",
        "    \n",
        "            targetLoss = torch.norm(head + rel - tail, 2).repeat(kg.n_ent, 1)\n",
        "    \n",
        "            tmpHead = head.repeat(kg.n_ent, 1)\n",
        "            tmpTail = tail.repeat(kg.n_ent, 1)\n",
        "            tmpRel  = rel.repeat(kg.n_ent, 1)\n",
        "    \n",
        "            tmpHloss = torch.norm(model.ent_embed.weight + tmpRel - tmpTail, 2, 1).view(-1, 1)\n",
        "            tmpTloss = torch.norm(tmpHead + tmpRel - model.ent_embed.weight, 2, 1).view(-1, 1)\n",
        "    \n",
        "            rankH = torch.nonzero(F.relu(targetLoss - tmpHloss)).size()[0]\n",
        "            rankT = torch.nonzero(F.relu(targetLoss - tmpTloss)).size()[0]\n",
        "    \n",
        "            ranks.append( (rankH + rankT + 2) / 2 )\n",
        "    \n",
        "    return sum(ranks) / len(ranks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hvVZLlLcS1z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0d8ff3bf-580a-4f8e-9fbf-778ae13ec9c2"
      },
      "source": [
        "fast_validate(model, kg_val, total_triplets, \"cuda\")"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7250.5785"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2fqQ5evggj9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}